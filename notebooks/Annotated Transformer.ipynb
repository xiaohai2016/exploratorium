{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Annotated Transformer\n",
    "\n",
    "Experimental codes written/copied from [here](http://nlp.seas.harvard.edu/2018/04/03/attention.html#embeddings-and-softmax) to fully understand the architecture of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to install necessary packages\n",
    "#!pip install numpy matplotlib spacy torchtext seaborn\n",
    "#!conda install pytorch torchvision -c pytorch -y\n",
    "#!pip install pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoder Module Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, generator, src_embed, tgt_embed):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.generator = generator\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        \n",
    "    def forward(self, src, src_mask, tgt, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(memory, src_mask, self.tgt_embed(tgt), tgt_mask)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_module, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_module, vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder Module\n",
    "\n",
    "It is composed of six identical EncoderLayers plus output normalization.\n",
    "\n",
    "It should be noted that, after the paper publication, an improvement was discovered, i.e. to normalize the input embedding vector in the encoder. The additional normalization step caused the encoding layers to start with normalization and end with residual addition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone(module, N):\n",
    "    # make N identical copies of module\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    # The encoder module is composed of six EncoderLayers\n",
    "    # followed by a layer normalization\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clone(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, src_embed, src_mask):\n",
    "        for encoderLayer in self.layers:\n",
    "            src_embed = encoderLayer(src_embed, src_mask)\n",
    "        # normalization is necessary since each EncoderLayer ends with residual addition.\n",
    "        return self.norm(src_embed)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_size, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a2 = nn.Parameter(torch.ones(feature_size))\n",
    "        self.b2 = nn.Parameter(torch.zeros(feature_size))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.a2 * (x - mean) / (std + self.eps) + self.b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    # Each encoder layer consists of the following:\n",
    "    # 1. input normalization\n",
    "    # 2. self attention & dropout\n",
    "    # 3. residual addition and normalization\n",
    "    # 4. a fully connected layer\n",
    "    # 5. residual addition\n",
    "    def __init__(self, d_model, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm_attn_input = LayerNorm(d_model)\n",
    "        self.norm_fc_input = LayerNorm(d_model)\n",
    "        self.size = d_model\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        input_x = x\n",
    "        \n",
    "        norm_x = self.norm_attn_input(input_x)\n",
    "        attn_out = self.dropout(self.self_attn(norm_x, norm_x, norm_x, mask))\n",
    "        input_x = input_x + attn_out\n",
    "        \n",
    "        norm_x = self.norm_fc_input(input_x)\n",
    "        fc_out = self.dropout(self.feed_forward(norm_x))\n",
    "        return input_x + fc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder consists of six layers of DecoderLayers plus output normalization\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clone(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, memory, src_mask, tgt, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(memory, src_mask, tgt, tgt_mask)\n",
    "        return self.norm(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the two sub-layers in each encoder layer, the decoder layer\n",
    "# has a third sub-layer, which performs multi-headed attention of the\n",
    "# encoder output\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, src_attn, self_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.norm_self_attn_input = LayerNorm(d_model)\n",
    "        self.norm_src_attn_input = LayerNorm(d_model)\n",
    "        self.norm_fc_input = LayerNorm(d_model)\n",
    "        self.src_attn = src_attn\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.size = d_model\n",
    "        \n",
    "    def forward(self, memory, src_mask, tgt, tgt_mask):\n",
    "        input_x = tgt\n",
    "        \n",
    "        norm_x = self.norm_self_attn_input(input_x)\n",
    "        self_attn_out = self.dropout(self.self_attn(norm_x, norm_x, norm_x, tgt_mask))\n",
    "        input_x = input_x + self_attn_out\n",
    "        \n",
    "        norm_x = self.norm_fc_input(input_x)\n",
    "        src_attn_out = self.dropout(self.src_attn(norm_x, memory, memory, src_mask))\n",
    "        input_x = input_x + src_attn_out\n",
    "        \n",
    "        norm_x = self.norm_src_attn_input(input_x)\n",
    "        fc_out = self.dropout(self.feed_forward(norm_x))\n",
    "        return input_x + fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build attention mask for the decoder outputs\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V, mask=None, dropout=None):\n",
    "    dim_k = Q.size(-1)\n",
    "    dot_prod = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        dot_prod = dot_prod.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(dot_prod, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, V), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    # Multi headed attention allows attention to different\n",
    "    # locations from different subspaces.\n",
    "    def __init__(self, head_count, model_size, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert model_size % head_count == 0\n",
    "        self.linears = clone(nn.Linear(model_size, model_size), 4)\n",
    "        self.head_count = head_count\n",
    "        self.head_size = model_size // head_count\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        if mask is not None:\n",
    "            # use the same mask for all heads\n",
    "            mask = mask.unsqueeze(1)\n",
    "        batch_size = Q.size(0)\n",
    "        q_trans, k_trans, v_trans = [\\\n",
    "                                     l(x).view(batch_size, -1, self.head_count, self.head_size).transpose(1, 2) \\\n",
    "                                     for l, x in zip(self.linears, (Q, K, V))]\n",
    "        x, self.attn = attention(q_trans, k_trans, v_trans, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head_count * self.head_size)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    # The position-wise feed-forward network has two linear models with Relu activation in between.\n",
    "    # the feed-forward network is shared among word embeddings of all positions.\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.lt1 = nn.Linear(d_model, d_ff)\n",
    "        self.lt2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lt2(self.dropout(F.relu(self.lt1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    # We use the usual learned embedding to convert the input/output of vocab size\n",
    "    # to vector of dimension d_model. \n",
    "    def __init__(self, vocab, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embeddings(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    # The position embedding are sine and cosine functions so that\n",
    "    # embedding vectors at p+k is a linear transformation from the\n",
    "    # embedding vectors at position p, and the transformation coef-\n",
    "    # ficients are indpendant of the value p.\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(-1 * torch.arange(0, d_model, 2, dtype=torch.float) * math.log(10000.0) / d_model)\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, d_model=512, d_ff=2048, head_count=8, dropout=0.1, layer_count=6):\n",
    "    mh_attn = MultiHeadedAttention(head_count, d_model, dropout=dropout)\n",
    "    feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout=dropout)\n",
    "    pe = PositionEmbedding(d_model, dropout=0.1)\n",
    "    encoder_layer = EncoderLayer(d_model, mh_attn, feed_forward, dropout=dropout)\n",
    "    encoder = Encoder(encoder_layer, layer_count)\n",
    "    decoder_layer = DecoderLayer(d_model, copy.deepcopy(mh_attn), copy.deepcopy(mh_attn), copy.deepcopy(feed_forward), dropout)\n",
    "    decoder = Decoder(decoder_layer, layer_count)\n",
    "    generator = Generator(d_model, tgt_vocab)\n",
    "    src_embed = nn.Sequential(Embeddings(src_vocab, d_model), pe)\n",
    "    tgt_embed = nn.Sequential(Embeddings(tgt_vocab, d_model), copy.deepcopy(pe))\n",
    "    model = EncoderDecoder(encoder, decoder, generator, src_embed, tgt_embed)\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(10, 10, 16, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching & Masking\n",
    "\n",
    "Each word in a target training sentence pays attention only\n",
    "    to words before it, and hence a target mask is created. Words\n",
    "    before the last becomes new training input (self.tgt). And\n",
    "    words after the first becomes the label (self.tgt_y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, tgt=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:,:-1]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.tgt_y = tgt[:,1:]\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum().type(torch.FloatTensor)\n",
    "            \n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Generic Training Loop\n",
    "\n",
    "Given a batch data iterator, a model and a loss function, run a training loop and report loss (and other information) every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(batch_iter, model, loss_compute, log_interval=50):\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    total_tokens = 0\n",
    "    start = time.time()\n",
    "    for i, batch in enumerate(batch_iter):\n",
    "        out = model.forward(batch.src, batch.src_mask, batch.tgt, batch.tgt_mask)\n",
    "        loss = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % log_interval == 0:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Batch Size Function\n",
    "\n",
    "The training utilizes [torchtext package](https://github.com/pytorch/text) for accessing datasets and preprocessing. For this purpose and dynamic batching, a batch size calculation function is to be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def my_batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    # torchtext automatically pads sequences to the maximum sequence length\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Optimizer\n",
    "\n",
    "The Adam optimizer with $\\beta_1=0.9$, $\\beta_2=0.98$ and $\\epsilon=10^{-9}$ is used with an adaptive learning rate. The learning rate is designed to linearly increase until a given $warmup\\_step$, and then decrease proportially to $\\sqrt{step\\_number}$. Additionally the learning rate is inversely proportionla to $\\sqrt{d_{model}}$. i.e.\n",
    "\n",
    "$lrate = factor \\times d_{model}^{-1/2} \\times \\min\\left\\{{step\\_number}^{-1/2},\\frac{step\\_number}{{warmup\\_step}^{3/2}}\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Label Smoothing\n",
    "\n",
    "We use label smoothing along with the KL divergence for the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    # size - the vocabulary size\n",
    "    # smoothing - the smooth factor\n",
    "    # padding_index - the index with padding\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.size = size\n",
    "        self.smoothing = smoothing\n",
    "        self.padding_index = padding_idx\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert self.size == x.size(1)\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        true_dist[:,self.padding_index] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_index)\n",
    "        if mask.size(0) > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data.item() * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a Simple Copy Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the simple copy task.\n",
    "TRY_COPY_TASK = True\n",
    "if TRY_COPY_TASK:\n",
    "    V = 11\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(V, V, layer_count=2)\n",
    "    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        run_epoch(data_gen(V, 30, 20), model, \n",
    "                  SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "        model.eval()\n",
    "        print(run_epoch(data_gen(V, 30, 5), model, \n",
    "                        SimpleLossCompute(model.generator, criterion, None)))\n",
    "\n",
    "    model.eval()\n",
    "    src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
    "    src_mask = Variable(torch.ones(1, 1, 10) )\n",
    "    print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tight batching\n",
    "\n",
    "Tight batching can improve training performance. We consolidate 100 batches from torchtext and sort by sample sizes to create tight batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TightBatchingIterator(data.Iterator):\n",
    "    \"\"\"\n",
    "    To contruct tight batching to improve training speed.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, sort_key=None, device=None,\n",
    "                 batch_size_fn=None, train=True,\n",
    "                 repeat=False, shuffle=None, sort=None,\n",
    "                 sort_within_batch=None):\n",
    "        super(TightBatchingIterator, self).__init__(dataset, batch_size, \\\n",
    "                sort_key=sort_key, device=device, \\\n",
    "                batch_size_fn=batch_size_fn, train=train, \\\n",
    "                repeat=repeat, shuffle=shuffle, sort=sort, \\\n",
    "                sort_within_batch=sort_within_batch)\n",
    "        self.batches = None\n",
    "\n",
    "    def create_batches(self):\n",
    "        \"\"\"\n",
    "        When training, we extract 100 batches from torchtext, sort them by size,\n",
    "        and then send them as individual batches.\n",
    "        For non-training, we simply perform a sort inside a batch.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            def pool(data_in, random_shuffler):\n",
    "                for macro_batch in data.batch(data_in, self.batch_size * 100):\n",
    "                    one_batch_iterator = data.batch(sorted(macro_batch, key=self.sort_key), \\\n",
    "                                           self.batch_size, self.batch_size_fn)\n",
    "                    for one_batch in random_shuffler(list(one_batch_iterator)):\n",
    "                        yield one_batch\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for one_batch in data.batch(self.data(), self.batch_size, self.batch_size_fn):\n",
    "                self.batches.append(sorted(one_batch, key=self.sort_key))\n",
    "\n",
    "def rebatch(padding_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, tgt = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    return Batch(src, tgt, padding_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU/CPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPULossCompute:\n",
    "    \"\"\"\n",
    "    Use mutlipe GPU for loss compute when available\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, criterion, opt=None, devices=None, chunk_size=5):\n",
    "        self.devices = devices\n",
    "        self.generator = generator\n",
    "        # criterion does not change during training\n",
    "        # replicate at object construction time\n",
    "        self.criterion = nn.parallel.replicate(criterion, devices=devices)\n",
    "        self.opt = opt\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __call__(self, out, target, normalize):\n",
    "        # Inference on the generator\n",
    "        generator = nn.parallel.replicate(self.generator, devices=self.devices)\n",
    "        out_scatter = nn.parallel.scatter(out, target_gpus=self.devices)\n",
    "        out_grad = [[] for _ in out_scatter]\n",
    "        target_scatter = nn.parallel.scatter(target, target_gpus=self.devices)\n",
    "\n",
    "        # Divide generating into chunks\n",
    "        chunk_size = self.chunk_size\n",
    "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
    "            # prediction distribution for a chunk\n",
    "            out_column = [[Variable(o[:, i:i + chunk_size].data, \\\n",
    "                            requires_grad=self.opt is not None)] \\\n",
    "                            for o in out_scatter]\n",
    "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
    "            # compute loss for a chunk\n",
    "            pred_label = [(g.contiguous().view(-1, g.size(-1)), \\\n",
    "                  t[:, i:i + chunk_size].contiguous().view(-1)) \\\n",
    "                     for g, t in zip(gen, target_scatter)]\n",
    "            loss = nn.parallel.parallel_apply(self.criterion, pred_label)\n",
    "            loss_compute = nn.parallel.gather(loss, target_device=self.devices[0])\n",
    "            loss_compute = loss_compute.sum()[0] / normalize\n",
    "            total += loss_compute.data[0]\n",
    "\n",
    "            if self.opt is not None:\n",
    "                loss_compute.backward()\n",
    "                for j, loss_compute in enumerate(loss):\n",
    "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
    "\n",
    "        # Backprop all loss through transformer.\n",
    "        if self.opt is not None:\n",
    "            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n",
    "            out1 = out\n",
    "            out2 = nn.parallel.gather(out_grad, \\\n",
    "                                    target_device=self.devices[0])\n",
    "            out1.backward(gradient=out2)\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return total * normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "\n",
    "class IWSLTTrainer:\n",
    "    \"\"\"The training class utilizes CPU/GPU for training\"\"\"\n",
    "    def __init__(self, use_gpu=False, devices=None, from_lang=\"de\", to_lang=\"en\"):\n",
    "        self.use_gpu = use_gpu\n",
    "        self.devices = devices\n",
    "\n",
    "        spacy_from = spacy.load(from_lang)\n",
    "        spacy_to = spacy.load(to_lang)\n",
    "        tokenizer_from = lambda x: [tok.text for tok in spacy_from.tokenizer(x)]\n",
    "        tokenizer_to = lambda x: [tok.text for tok in spacy_to.tokenizer(x)]\n",
    "\n",
    "        bos_word = '<s>'\n",
    "        eos_word = '</s>'\n",
    "        blank_word = '<blank>'\n",
    "        src_field = data.Field(tokenize=tokenizer_from, pad_token=blank_word)\n",
    "        tgt_field = data.Field(tokenize=tokenizer_to, init_token=bos_word, \\\n",
    "                     eos_token=eos_word, pad_token=blank_word)\n",
    "        max_len = 100\n",
    "        self.train, self.val, self.test = datasets.IWSLT.splits(\n",
    "            exts=('.' + from_lang, '.' + to_lang), fields=(src_field, tgt_field),\n",
    "            filter_pred=lambda x: len(vars(x)['src']) <= max_len and \\\n",
    "                          len(vars(x)['trg']) <= max_len)\n",
    "\n",
    "        min_freq = 2\n",
    "        src_field.build_vocab(self.train.src, min_freq=min_freq)\n",
    "        tgt_field.build_vocab(self.train.trg, min_freq=min_freq)\n",
    "        self.pad_idx = tgt_field.vocab.stoi[blank_word]\n",
    "        self.tgt_field = tgt_field\n",
    "        self.src_field = src_field\n",
    "\n",
    "        self.model = make_model(len(src_field.vocab), len(tgt_field.vocab), layer_count=6)\n",
    "        if use_gpu:\n",
    "            self.model.cuda()\n",
    "        self.criterion = LabelSmoothing(size=len(tgt_field.vocab), \\\n",
    "                    padding_idx=self.pad_idx, smoothing=0.1)\n",
    "        if use_gpu:\n",
    "            self.criterion.cuda()\n",
    "\n",
    "        batch_size = 12000\n",
    "        self.train_iter = TightBatchingIterator(self.train, batch_size=batch_size, device=None, \\\n",
    "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)), \\\n",
    "                            batch_size_fn=my_batch_size_fn, train=True)\n",
    "        self.valid_iter = TightBatchingIterator(self.val, batch_size=batch_size, device=None, \\\n",
    "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)), \\\n",
    "                            batch_size_fn=my_batch_size_fn, train=False)\n",
    "        if use_gpu:\n",
    "            self.model_par = nn.DataParallel(self.model, device_ids=devices)\n",
    "\n",
    "        self.model_opt = NoamOpt(self.model.src_embed[0].d_model, 1, 2000, \\\n",
    "            torch.optim.Adam(self.model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "    def run_training(self, epoch_count, log_interval=50):\n",
    "        '''Run training for multiple epochs'''\n",
    "        for _ in range(epoch_count):\n",
    "            if self.use_gpu:\n",
    "                self.model_par.train()\n",
    "                run_epoch((rebatch(self.pad_idx, b) for b in self.train_iter), \\\n",
    "                      self.model_par, \\\n",
    "                      MultiGPULossCompute(self.model.generator, self.criterion, \\\n",
    "                                          devices=self.devices, opt=self.model_opt), \\\n",
    "                      log_interval=log_interval)\n",
    "                self.model_par.eval()\n",
    "                loss = run_epoch((rebatch(self.pad_idx, b) for b in self.valid_iter), \\\n",
    "                              self.model_par, \\\n",
    "                              MultiGPULossCompute(self.model.generator, self.criterion, \\\n",
    "                                  devices=self.devices, opt=None), \\\n",
    "                              log_interval=log_interval)\n",
    "                print(loss)\n",
    "            else:\n",
    "                self.model.train()\n",
    "                run_epoch((rebatch(self.pad_idx, b) for b in self.train_iter), \\\n",
    "                      self.model, \\\n",
    "                      SimpleLossCompute(self.model.generator, self.criterion, opt=self.model_opt), \\\n",
    "                      log_interval=log_interval)\n",
    "                self.model.eval()\n",
    "                loss = run_epoch((rebatch(self.pad_idx, b) for b in self.valid_iter), \\\n",
    "                              self.model, \\\n",
    "                              MultiGPULossCompute(self.model.generator, self.criterion, \\\n",
    "                                  devices=self.devices, opt=None), \\\n",
    "                              log_interval=log_interval)\n",
    "                print(loss)\n",
    "\n",
    "    def run_validating(self):\n",
    "        \"\"\"\n",
    "        Once trained we can decode the model to produce a set of translations.\n",
    "        Here we simply translate the first sentence in the validation set. This\n",
    "        dataset is pretty small so the translations with greedy search are\n",
    "        reasonably accurate.\n",
    "        \"\"\"\n",
    "        for _, batch in enumerate(self.valid_iter):\n",
    "            src = batch.src.transpose(0, 1)[:1]\n",
    "            src_mask = (src != self.src_field.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
    "            out = greedy_decode(self.model, src, src_mask, \\\n",
    "                        max_len=60, start_symbol=self.tgt_field.vocab.stoi[\"<s>\"])\n",
    "            print(\"Translation:\", end=\"\\t\")\n",
    "            for j in range(1, out.size(1)):\n",
    "                sym = self.tgt_field.vocab.itos[out[0, j]]\n",
    "                if sym == \"</s>\":\n",
    "                    break\n",
    "                print(sym, end=\" \")\n",
    "            print()\n",
    "            print(\"Target:\", end=\"\\t\")\n",
    "            for j in range(1, batch.trg.size(0)):\n",
    "                sym = self.tgt_field.vocab.itos[batch.trg.data[j, 0]]\n",
    "                if sym == \"</s>\":\n",
    "                    break\n",
    "                print(sym, end=\" \")\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_cpu():\n",
    "    \"\"\"Run training for IWSLT on CPU for debugging purpose\"\"\"\n",
    "    trainer = IWSLTTrainer()\n",
    "    trainer.run_training(10, log_interval=1)\n",
    "\n",
    "train_on_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
